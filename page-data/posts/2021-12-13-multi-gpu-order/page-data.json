{"componentChunkName":"component---src-templates-post-js","path":"/posts/2021-12-13-multi-gpu-order/","result":{"data":{"site":{"siteMetadata":{"title":"MJUN Tech Note","author":"Junya Morioka"}},"markdownRemark":{"id":"17d923bd-4d4a-5f67-929d-072db078c7b6","html":"<p>マルチ GPU を搭載したマシンで，<code class=\"language-text\">nvidia-smi</code>で見える GPU ID と，\nPyTorch や Tensorflow，<code class=\"language-text\">CUDA_VISIBLE_DEVICES</code>で指定する GPU ID が異なる現象についてのメモ．</p>\n<p>結論から言うと，<code class=\"language-text\">nvidia-smi</code>では PCI BUS 順に並ぶが，\nCUDA ではデフォルトでは早い順(<code class=\"language-text\">FASTEST_FIRST</code>)に GPU の ID が振られることが原因だった．</p>\n<p>A6000 x 2，RTX 3090 x 2 が刺さっているマシンを想定し，\n<code class=\"language-text\">nvidia-smi</code>で並ぶ GPU の順番が以下のようだったとする．</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token number\">0</span>: RTX <span class=\"token number\">3090</span>\n<span class=\"token number\">1</span>: A6000\n<span class=\"token number\">2</span>: A6000\n<span class=\"token number\">3</span>: RTX <span class=\"token number\">3090</span></code></pre></div>\n<p>すると，PyTorch 側では以下のように並ぶ．</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token number\">0</span>: A6000  <span class=\"token comment\"># (nvidia-smiでは1)</span>\n<span class=\"token number\">1</span>: A6000  <span class=\"token comment\"># (nvidia-smiでは2)</span>\n<span class=\"token number\">2</span>: RTX <span class=\"token number\">3090</span>  <span class=\"token comment\"># (nvidia-smiでは0)</span>\n<span class=\"token number\">3</span>: RTX <span class=\"token number\">3090</span>  <span class=\"token comment\"># (nvidia-smiでは3)</span></code></pre></div>\n<p>PyTorch 側でも<code class=\"language-text\">nvidia-smi</code>(PCI バス順)の順番で\nGPU の ID を振りたい時は以下のように環境変数を設定する．</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">CUDA_DEVICE_ORDER</span><span class=\"token operator\">=</span><span class=\"token string\">\"PCI_BUS_ID\"</span>\n<span class=\"token comment\"># または，</span>\n<span class=\"token assign-left variable\">CUDA_DEVICE_ORDER</span><span class=\"token operator\">=</span><span class=\"token string\">\"PCI_BUS_ID\"</span> <span class=\"token assign-left variable\">CUDA_VISIBLE_DEVICES</span><span class=\"token operator\">=</span><span class=\"token string\">\"0\"</span> python train.py</code></pre></div>\n<p>これで，上記のマシンでは，<code class=\"language-text\">nvidia-smi</code>の順番となり，RTX 3090 が ID=0 となる．</p>\n<p>コード上で変更するには，以下のようにする(Python)</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n\nos<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">\"CUDA_DEVICE_ORDER\"</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">\"PCI_BUS_ID\"</span>\nos<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">\"CUDA_VISIBLE_DEVICES\"</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">\"0, 3\"</span></code></pre></div>\n<p>ちなみに，PyTorch では，以下のコードで GPU の情報を表示できる</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n\n<span class=\"token comment\"># os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"</span>\n<span class=\"token comment\"># os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"</span>\n\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    info <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>get_device_properties<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"CUDA:</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>i<span class=\"token punctuation\">}</span></span><span class=\"token string\"> </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>info<span class=\"token punctuation\">.</span>name<span class=\"token punctuation\">}</span></span><span class=\"token string\">, </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>info<span class=\"token punctuation\">.</span>total_memory <span class=\"token operator\">/</span> <span class=\"token number\">1024</span> <span class=\"token operator\">**</span> <span class=\"token number\">2</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">MB\"</span></span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>環境変数の設定は，torch.cuda 周りを呼び出す前に行わければならないのに注意．</p>\n<p>ここまで書いてきて，常に<code class=\"language-text\">PCI_BUS_ID</code>順にすればいいじゃんと思うかもしれないが，\nMulti-GPU で異なる GPU を混ぜて推論するときには，\nGPU ID=0 で損失計算などを行うので，GPU ID=0 の GPU の計算が早いほうが嬉しい．\nそのため，早い順に GPU が並ぶのも利点が一応ある．</p>\n<h3 id=\"参考\">参考</h3>\n<p><a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars</a></p>","tableOfContents":"","excerpt":"マルチ GPU を搭載したマシンで，で見える GPU ID と，\nPyTorch や Tensorflow，で指定する GPU ID が異なる現象についてのメモ． 結論から言うと，では PCI BUS 順に並ぶが，\nCUDA ではデフォルトでは早い順()に GPU の ID…","frontmatter":{"title":"GPUを複数搭載した計算機でGPUの順番を設定する","date":"2021.12.13","update":"2022.04.24","category":"PyTorch","tags":["PyTorch","Tensorflow","Server"]},"fields":{"slug":"/posts/2021-12-13-multi-gpu-order/"}}},"pageContext":{"id":"17d923bd-4d4a-5f67-929d-072db078c7b6"}},"staticQueryHashes":["1123391092"],"slicesMap":{}}