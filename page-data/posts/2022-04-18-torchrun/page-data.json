{"componentChunkName":"component---src-templates-post-js","path":"/posts/2022-04-18-torchrun/","result":{"data":{"site":{"siteMetadata":{"title":"MJUN Tech Note","author":"Junya Morioka"}},"markdownRemark":{"id":"b337d360-ea30-591a-b6f8-30025aac8fb6","html":"<p>機械学習ライブラリの PyTorch には、複数のマシン・GPU で\n学習を行う方法がいくつか用意されている。<br>\nこの記事を書いている現在、PyTorch の stable version は<code class=\"language-text\">1.11.0</code>であるが、\n実行方法が、直近の version<code class=\"language-text\">1.9.0</code>と<code class=\"language-text\">1.10.0</code>で変更・追加があったのでまとめる。</p>\n<h2 id=\"dataparallel-と-distributeddataparallel\">DataParallel と DistributedDataParallel</h2>\n<p>PyTorch で複数の GPU を用いた Training の実装方法は 2 つある。</p>\n<ol>\n<li><code class=\"language-text\">torch.nn.DataParallel</code></li>\n<li><code class=\"language-text\">torch.nn.DistributedDataParallel</code></li>\n</ol>\n<p>この２つの違いは、複数の GPU に割り当てられるCPUコアが\n全体で1つか各GPUに複数かである。</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f267346af6b6d553c5249a7e92c166b6/f43b1/torch_dist.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 55.99999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAEDBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHtTaQB/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAECEBEx/9oACAEBAAEFAp6S1C4Ov//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAABICH/2gAIAQEABj8CRk//xAAaEAADAAMBAAAAAAAAAAAAAAAAAREhMUFR/9oACAEBAAE/IWRwuwzIbwaL4QRNmj//2gAMAwEAAgADAAAAEEPv/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQMBAT8Qh//EABYRAQEBAAAAAAAAAAAAAAAAAAEhEP/aAAgBAgEBPxBG3P/EABoQAQEBAAMBAAAAAAAAAAAAAAERACExUUH/2gAIAQEAAT8QrG0rl694zC4LOVreu81cRRTIthfZhFM/Bv/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"nvidia-diff-parallel\"\n        title=\"\"\n        src=\"/static/f267346af6b6d553c5249a7e92c166b6/4b190/torch_dist.jpg\"\n        srcset=\"/static/f267346af6b6d553c5249a7e92c166b6/e07e9/torch_dist.jpg 200w,\n/static/f267346af6b6d553c5249a7e92c166b6/066f9/torch_dist.jpg 400w,\n/static/f267346af6b6d553c5249a7e92c166b6/4b190/torch_dist.jpg 800w,\n/static/f267346af6b6d553c5249a7e92c166b6/e5166/torch_dist.jpg 1200w,\n/static/f267346af6b6d553c5249a7e92c166b6/f43b1/torch_dist.jpg 1426w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>上記の図<sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup>のように、Python の GIL の都合もあり、\n<code class=\"language-text\">DistributedDataParallel</code>を使ったほうが各 GPU に個別の CPU コアを割り当てられるので、\nリソースを存分に使うことができる。\nまた、複数のマシン(Multi-node)で実行できるのも強みである。\n実際、公式ドキュメント<sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup>でも<code class=\"language-text\">DistributedDataParallel</code>が勧められている。</p>\n<p>ここまでくると、<code class=\"language-text\">DataParallel</code>のメリットが感じられないが、実装の違いを\n見ると利点が見えてくる。</p>\n<p>まず、<code class=\"language-text\">DataParallel</code>の実装は以下である。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n\nmodel <span class=\"token operator\">=</span> hoge<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">+</span> model <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>DataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> device_ids<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>上記のように既存のモデル(<code class=\"language-text\">torch.nn.Model</code>)に対して、<code class=\"language-text\">torch.nn.DataParallel</code>をラップするだけで\n実装でき、既存のコードを 1 行変更するだけで実装することができる。</p>\n<p>次に、<code class=\"language-text\">DistributedDataParallel</code>の実装例を確認する。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>distributed <span class=\"token keyword\">as</span> dist\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>parallel <span class=\"token keyword\">import</span> DistributedDataParallel <span class=\"token keyword\">as</span> DDP\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>distributed <span class=\"token keyword\">import</span> DistributedSampler\n\n<span class=\"token comment\"># どのGPUプロセス番号かがLocal Rank</span>\n<span class=\"token comment\"># GPU ID = 1の時、local_rank=1</span>\nlocal_rank <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>getenv<span class=\"token punctuation\">(</span><span class=\"token string\">'LOCAL_RANK'</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 通信方法の規定とプロセスグループの初期化</span>\ndist<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span> init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">)</span>\n\ndataset <span class=\"token operator\">=</span> Dataset<span class=\"token punctuation\">(</span>hoge<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># DistributedSamplerを使う</span>\nsampler <span class=\"token operator\">=</span> DistributedSampler<span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">,</span> rank<span class=\"token operator\">=</span>local_rank<span class=\"token punctuation\">)</span>\ndataloaders <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>DataLoader<span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">,</span>\n                                          batch_size<span class=\"token operator\">=</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span>\n                                          sampler<span class=\"token operator\">=</span>Distributed<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># DistributedDataParallelでラップ</span>\nmodel <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span>fuga<span class=\"token punctuation\">)</span>\nmodel <span class=\"token operator\">=</span> DDP<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># ...</span>\n\n<span class=\"token comment\"># Training終了</span>\ndist<span class=\"token punctuation\">.</span>destroy_process_group<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><code class=\"language-text\">DistributedDataParallel</code>では、シングル・マルチマシンの場合も同じ書き方ができるように\n設計されているため、新たに実装する部分が多い。\nマルチプロセスになった分、自分が今どのプロセスにいるのかを意識しながら実装を進める必要がある。</p>\n<p>上記の通り、<code class=\"language-text\">DataParallel</code>は 1 行で既存のコードを変更することができるが、\n<code class=\"language-text\">DistributedDataParallel</code>は多少の追加実装が必要になる。\n手軽に複数 GPU での Training を試したい場合は、<code class=\"language-text\">DataParallel</code>を用いるとよい。</p>\n<p>次章では新たに追加された<code class=\"language-text\">torchrun</code>について議論するため、以下からは<code class=\"language-text\">DistributedDataParallel</code>\nを用いた場合について考える。</p>\n<h2 id=\"distributeddataparallel-の実行方法\">DistributedDataParallel の実行方法</h2>\n<p>DistributedDataParallel の実行方法は、大きく分けて以下の２つある。</p>\n<ol>\n<li>特定の関数について GPU 並列化を行う方法(<code class=\"language-text\">mp.spawn</code>)</li>\n<li>スクリプトごと GPU 並列化する方法(<code class=\"language-text\">torchrun</code>, <code class=\"language-text\">torch.distributed.run</code>, <code class=\"language-text\">torch.distributed.launch</code>)</li>\n</ol>\n<h3 id=\"1-関数について並列化\">1. 関数について並列化</h3>\n<p>1 の関数ごとに並列化する方法は、コード内で Training を行う関数を書いて、使用する GPU の数や\n通信方法もコード内で設定して実行することができる。\nつまり、シングル GPU であっても、複数 GPU のコードであっても<code class=\"language-text\">python train.py</code>と、\n同じコマンドの実行で学習が行える。<br>\n実装例は PyTorch 公式の ImageNet 学習の実装に書かれている。</p>\n<p><a href=\"https://github.com/pytorch/examples/tree/main/imagenet\">https://github.com/pytorch/examples/tree/main/imagenet</a></p>\n<p>2 の方法と大きく異なる部分が、以下の部分である。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>multiprocessing <span class=\"token keyword\">as</span> mp\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">train</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> hoge<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    dist<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span> init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    mp<span class=\"token punctuation\">.</span>spawn<span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">,</span> nprocs<span class=\"token operator\">=</span>ngpus_per_node<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>hoge<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>上記の通り、コード自体は Python 標準モジュールの multiprocessing と変わりない。<br>\nしかし、標準モジュールは CUDA Initialized を複数行ってしまい、エラーが発生するため、\nmultiprocessing モジュールをラップした、<code class=\"language-text\">torch.multiprocessing</code>を使用する。</p>\n<h3 id=\"2-スクリプトごと並列化\">2. スクリプトごと並列化</h3>\n<p>2 の方法については PyTorch のバージョンによって実行方法が異なっており、\nversion 1.9.0 以前は、<br>\n<code class=\"language-text\">python -m torch.distributed.launch --nproc_per_node=4 --nnodes=1 --node_rank 0 train.py</code><br>\nで実行されていたが、version 1.9.0 以降は TorchElastic が追加された影響で<br>\n<code class=\"language-text\">python -m torch.distributed.run --nproc_per_node=4 --nnodes=1 --node_rank 0 train.py</code><br>\nでも実行できる。<br>\nまた、<code class=\"language-text\">torch.distributed.launch</code>の super set として、<code class=\"language-text\">torchrun</code>が Version 1.10.0 から提供されている。</p>\n<p>ここでは従来の方法である、<code class=\"language-text\">torch.distributed.launch</code>と<code class=\"language-text\">torch.distributed.run</code>について述べる。</p>\n<p><code class=\"language-text\">torch.distributed.launch</code>と<code class=\"language-text\">torch.distributed.run</code>の場合、\n実行スクリプトの<code class=\"language-text\">train.py</code>にはコマンドライン引数として\n<code class=\"language-text\">--local_rank</code>を受け取れるように実装する必要がある。下に例を示す。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> argparse\nparser <span class=\"token operator\">=</span> argparse<span class=\"token punctuation\">.</span>ArgumentParser<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">\"--local_rank\"</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">type</span><span class=\"token operator\">=</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span>\nargs <span class=\"token operator\">=</span> parser<span class=\"token punctuation\">.</span>parse_args<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nlocal_rank <span class=\"token operator\">=</span> args<span class=\"token punctuation\">.</span>local_rank</code></pre></div>\n<p>これ以外の実装は 1 の関数ごとに multiprocessing する場合と変わらない。</p>\n<h3 id=\"1-と-2-の実行方法の違いについて\">1 と 2 の実行方法の違いについて</h3>\n<p>1 の関数を multiprocessing する方法と、スクリプト自体を multiprocessing する方法は、\n<a href=\"%5E3\">こちらの公式フォーラム</a>\n<sup id=\"fnref-3\"><a href=\"#fn-3\" class=\"footnote-ref\">3</a></sup>でも言及されているように、\n<code class=\"language-text\">multiprocessing(1) vs subprocess(2)</code>の違いといえる。</p>\n<p>Github の Issue<sup id=\"fnref-4\"><a href=\"#fn-4\" class=\"footnote-ref\">4</a></sup> <sup id=\"fnref-5\"><a href=\"#fn-5\" class=\"footnote-ref\">5</a></sup>では、1 の方法が GPU への転送速度の関係で遅いという報告もある。\n長い時間の学習では無視できるようだが、参考としておきたい。</p>\n<h2 id=\"新しい実行方法-torchrun\">新しい実行方法 <code class=\"language-text\">torchrun</code></h2>\n<p>PyTorch の Version 1.10.0 から、<code class=\"language-text\">torch.distributed.launch</code>の super set として、<code class=\"language-text\">torchrun</code>が登場している。</p>\n<p>公式ドキュメント<sup id=\"fnref-6\"><a href=\"#fn-6\" class=\"footnote-ref\">6</a></sup>にわかりやすい移行手順があるので、一読をお勧めする。</p>\n<p>具体的には、実行コマンドが以下のように変更され、</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token comment\"># use_envはLOCAL RANKをargparseではなく、</span>\n<span class=\"token comment\"># 環境変数から受け取るオプション</span>\npython <span class=\"token parameter variable\">-m</span> torch.distributed.launch <span class=\"token parameter variable\">--use_env</span> train_script.py\n\ntorchrun train_script.py</code></pre></div>\n<p>argparse で受け取っていた local rank を環境変数から受け取るようになる。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># torch.distributed.launch</span>\n<span class=\"token keyword\">import</span> argparse\nparser <span class=\"token operator\">=</span> argparse<span class=\"token punctuation\">.</span>ArgumentParser<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">\"--local_rank\"</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">type</span><span class=\"token operator\">=</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span>\nargs <span class=\"token operator\">=</span> parser<span class=\"token punctuation\">.</span>parse_args<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nlocal_rank <span class=\"token operator\">=</span> args<span class=\"token punctuation\">.</span>local_rank\n\n<span class=\"token comment\"># torchrun</span>\n<span class=\"token keyword\">import</span> os\nlocal_rank <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">\"LOCAL_RANK\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>ほとんど使用感は変わりないが、わざわざ argparse で引数の受取先を作らなくてよくなったのは、\nコマンドライン引数の名前空間を汚されなくて済むので利点がある。<br>\n例えば、Facebook 謹製の設定管理ライブラリの\n<a href=\"https://hydra.cc/\">Hydra</a> <sup id=\"fnref-7\"><a href=\"#fn-7\" class=\"footnote-ref\">7</a></sup>を使っている場合、argparse と併用ができないので、\ntorchrun で環境変数を経由するメリットがある。<br>\n(ただし、ここ<sup id=\"fnref-8\"><a href=\"#fn-8\" class=\"footnote-ref\">8</a></sup> <sup id=\"fnref-9\"><a href=\"#fn-9\" class=\"footnote-ref\">9</a></sup>で議論されているように、output 周りが conflict する問題があるので、\n今後の動向に注目するべきである。)</p>\n<p>ここ<sup id=\"fnref-10\"><a href=\"#fn-10\" class=\"footnote-ref\">10</a></sup>で、書かれているように<code class=\"language-text\">torch.distributed.launch</code>\nは将来的に deprecated したいようなので、今後は torchrun で実装していくべきだろう。</p>\n<h2 id=\"参考\">参考</h2>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-1\"><a href=\"https://qiita.com/sugulu_Ogawa_ISID/items/62f5f7adee083d96a587#4-multi-gpu%E3%81%AE%E8%A8%AD%E5%AE%9A\">https://qiita.com/sugulu_Ogawa_ISID/items/62f5f7adee083d96a587#4-multi-gpu%E3%81%AE%E8%A8%AD%E5%AE%9A</a><a href=\"#fnref-1\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-2\"><a href=\"https://pytorch.org/docs/1.11/notes/cuda.html#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel\">https://pytorch.org/docs/1.11/notes/cuda.html#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel</a><a href=\"#fnref-2\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-3\"><a href=\"https://discuss.pytorch.org/t/torch-distributed-launch-vs-torch-multiprocessing-spawn/95738\">https://discuss.pytorch.org/t/torch-distributed-launch-vs-torch-multiprocessing-spawn/95738</a><a href=\"#fnref-3\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-4\"><a href=\"https://github.com/pytorch/pytorch/issues/47587\">https://github.com/pytorch/pytorch/issues/47587</a><a href=\"#fnref-4\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-5\"><a href=\"https://github.com/NVIDIA/apex/issues/549\">https://github.com/NVIDIA/apex/issues/549</a><a href=\"#fnref-5\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-6\"><a href=\"https://pytorch.org/docs/1.11/elastic/run.html\">https://pytorch.org/docs/1.11/elastic/run.html</a><a href=\"#fnref-6\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-7\"><a href=\"https://hydra.cc/\">https://hydra.cc/</a><a href=\"#fnref-7\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-8\"><a href=\"https://github.com/facebookresearch/hydra/pull/2119\">https://github.com/facebookresearch/hydra/pull/2119</a><a href=\"#fnref-8\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-9\"><a href=\"https://github.com/facebookresearch/hydra/issues/2038\">https://github.com/facebookresearch/hydra/issues/2038</a><a href=\"#fnref-9\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-10\"><a href=\"https://pytorch.org/docs/1.11/distributed.html#launch-utility\">https://pytorch.org/docs/1.11/distributed.html#launch-utility</a><a href=\"#fnref-10\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>","tableOfContents":"<ul>\n<li><a href=\"#dataparallel-%E3%81%A8-distributeddataparallel\">DataParallel と DistributedDataParallel</a></li>\n<li><a href=\"#distributeddataparallel-%E3%81%AE%E5%AE%9F%E8%A1%8C%E6%96%B9%E6%B3%95\">DistributedDataParallel の実行方法</a></li>\n<li><a href=\"#%E6%96%B0%E3%81%97%E3%81%84%E5%AE%9F%E8%A1%8C%E6%96%B9%E6%B3%95-torchrun\">新しい実行方法 <code class=\"language-text\">torchrun</code></a></li>\n<li><a href=\"#%E5%8F%82%E8%80%83\">参考</a></li>\n</ul>","excerpt":"機械学習ライブラリの PyTorch には、複数のマシン・GPU で\n学習を行う方法がいくつか用意されている。 この記事を書いている現在、PyTorch の stable version はであるが、\n実行方法が、直近の version…","frontmatter":{"title":"PyTorchのMultiGPUの概要 【DataParallel, DistributedDataParallel, torchrun】","date":"2022.04.18","update":"2022.07.27","category":"PyTorch","tags":["PyTorch","Python"]},"fields":{"slug":"/posts/2022-04-18-torchrun/"}}},"pageContext":{"id":"b337d360-ea30-591a-b6f8-30025aac8fb6"}},"staticQueryHashes":["1123391092"],"slicesMap":{}}