{"componentChunkName":"component---src-templates-post-js","path":"/posts/2023-01-21_torch-multi-gpu-vram-imbalance/","result":{"data":{"site":{"siteMetadata":{"title":"MJUN Tech Note","author":"Junya Morioka"}},"markdownRemark":{"id":"84084fa5-570f-5237-9f31-61fbd5c22dec","html":"<p>こんにちは．今回はPyTorchのTipsで，Multi GPU Training時に設定不足などで起こる，\nGPU間でのVRAMの使用量の偏りを直す方法を紹介します．</p>\n<p>基本的にGPU間のVRAM使用量の偏りが起こる時は，実装のミスが多いです．</p>\n<p>PyTorchで重みやモデルやテンソルをGPUへ転送する時に，<code class=\"language-text\">.to(\"cuda\")</code>や<code class=\"language-text\">.cuda()</code>\nで転送する人が多いと思いますが，GPUのIDを何も指定しないと，\nIDが0のGPUへ転送されます．</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nt <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">256</span><span class=\"token punctuation\">,</span> <span class=\"token number\">256</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">\"cuda\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Distribute Data Parallel(DDP)やData Parallel(DP)では，\n学習を実行するスクリプトのマスターノードからは，使用するGPUが全て見えている状態です．</p>\n<p>しかし，テンソルやモデルは，それぞれのGPUに転送する必要があるため，GPUのIDを指定しないと全てID 0のGPUに転送されて\nしまい，VRAM使用量の偏りを起こしてしまいます．</p>\n<p><code class=\"language-text\">.to(\"cuda:1\")</code>や<code class=\"language-text\">.cuda(6)</code>といった書き方で転送するGPUを指定することもできますが，\nこの方法だと，Single GPUとMulti GPUでの学習でコードを使いまわすことが難しくなってしまいます．</p>\n<p>そこで，<code class=\"language-text\">torch.cuda.set_device()</code>を設定することで，IDを指定しなかった時の転送先GPUを変更することができます．</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Set Local Rank for Multi GPU Training</span>\nrank <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token string\">\"LOCAL_RANK\"</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Set Device</span>\n<span class=\"token keyword\">if</span> cfg<span class=\"token punctuation\">.</span>CPU<span class=\"token punctuation\">:</span>\n    device <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span><span class=\"token string\">\"cpu\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">elif</span> rank <span class=\"token operator\">!=</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n    device <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">)</span>\n    torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>set_device<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n    device <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n    torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>set_device<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>上記のコードでは，変数<code class=\"language-text\">rank</code>でDDPかSingle GPUでの学習かを判断することができます．\nDDP時には環境変数の<code class=\"language-text\">LOCAL_RANK</code>に計算機の中で実行される学習プロセスの\nIDが振られており，これをそのままGPUのIDに利用することができます．</p>\n<p>Single GPUでの学習時には上記の実装だと，<code class=\"language-text\">rank</code>は-1になるため\nDDPとの場合分けも容易になります．</p>\n<p>上記の実装と環境変数<code class=\"language-text\">CUDA_VISIBLE_DEVICES</code>を用いれば，容易に使用するGPUを\n選択することができるかと思います．</p>\n<h3 id=\"参考\">参考</h3>\n<p><a href=\"https://discuss.pytorch.org/t/extra-10gb-memory-on-gpu-0-in-ddp-tutorial/118113\">https://discuss.pytorch.org/t/extra-10gb-memory-on-gpu-0-in-ddp-tutorial/118113</a></p>","tableOfContents":"","excerpt":"こんにちは．今回はPyTorchのTipsで，Multi GPU Training時に設定不足などで起こる，\nGPU間でのVRAMの使用量の偏りを直す方法を紹介します． 基本的にGPU間のVRAM使用量の偏りが起こる時は，実装のミスが多いです． PyTorch…","frontmatter":{"title":"PyTorch DDPでマルチGPU学習時にVRAMの偏りが発生した時","date":"2023.01.21","update":"2023.01.21","category":"PyTorch","tags":["PyTorch"]},"fields":{"slug":"/posts/2023-01-21_torch-multi-gpu-vram-imbalance/"}}},"pageContext":{"id":"84084fa5-570f-5237-9f31-61fbd5c22dec"}},"staticQueryHashes":["1123391092"],"slicesMap":{}}