---
title: "HuggingFace datasetsã®Builder classã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªä½œã™ã‚‹"
tags: ["HuggingFace", "dataset"]
category: "HuggingFace"
date: 2024-09-22
update: 2024-09-22
# for Zenn
type: tech
emoji: ğŸ˜–
topics: [None]
published: true
---

ã“ã‚“ã«ã¡ã¯ã€‚ä»Šå›ã¯HuggingFace datasetsã‹ã‚‰
å‘¼ã³å‡ºã›ã‚‹å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªä½œã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã“ã®è¨˜äº‹ã§ã¯ç‰¹ã«Builder classã‚’ç”¨ã„ãŸæ–¹æ³•ã«ã¤ã„ã¦æ›¸ã„ã¦ã„ãã¾ã™ã€‚
ã‚„ã£ã¦ã„ã‚‹ã“ã¨ã¯ä»¥ä¸‹ã¨åŒæ§˜ã§ã™ã€‚

<https://huggingface.co/docs/datasets/dataset_script>

<https://github.com/huggingface/datasets/tree/main/templates>

## å‰æ: HuggingFaceã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹æ–¹æ³•

HuggingFaceã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªä½œã™ã‚‹æ–¹æ³•ã¯ã€å¤§ããåˆ†ã‘ã¦ä»¥ä¸‹ã®3ã¤ã‚ã‚Šã¾ã™ã€‚

1. ã‚ã‚‰ã‹ã˜ã‚ç‰¹å®šã®æ§‹é€ ã®ãƒ•ã‚¡ã‚¤ãƒ« or ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç”¨æ„ã—ã¦`datasets.load_dataset`ã‚’ä½¿ã†æ–¹æ³•
2. dictã‚„generatorã‚’å®šç¾©ã—ã¦`datasets.Dataset.from_*`é–¢æ•°ã‚’åˆ©ç”¨ã™ã‚‹æ–¹æ³•
3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã‚’ã‚³ãƒ¼ãƒ‰ã§å®šç¾©ã™ã‚‹`datasets.DatasetBuilder`ã‚¯ãƒ©ã‚¹ã‚’åˆ©ç”¨ã™ã‚‹æ–¹æ³•(ä»Šå›ç´¹ä»‹ã™ã‚‹æ–¹æ³•)

### 1. ã‚ã‚‰ã‹ã˜ã‚ç‰¹å®šã®æ§‹é€ ã®ãƒ•ã‚¡ã‚¤ãƒ« or ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç”¨æ„ã—ã¦datasets.load_datasetã‚’ä½¿ã†æ–¹æ³•

ã¾ãšã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚€ã“ã‚€æ–¹æ³•ã¯ä»¥ä¸‹ã®ãƒªãƒ³ã‚¯ã®ã‚ˆã†ã«ã€ã‚ã‚‰ã‹ã˜ã‚ç”¨æ„ã—ãŸ`csv, json`ãªã©ã®å½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãã®ã¾ã¾load_dataseté–¢æ•°ã§èª­ã¿è¾¼ã¿ã¾ã™ã€‚

<https://huggingface.co/docs/datasets/loading#local-and-remote-files>

ä¾‹ãˆã°csvå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ãªã‚‰ä»¥ä¸‹ã®ã‚ˆã†ã«èª­ã¿è¾¼ã¿ã¾ã™ã€‚

```python
import datasets as ds

dataset = ds.load_dataset(
    "csv",
    data_files={"train": ["my_train_file_1.csv", "my_train_file_2.csv"], "test": "my_test_file.csv"}
)
```

ã‚ã‚‰ã‹ã˜ã‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ã—ã¦ãŠãã ã‘ãªã®ã§ç°¡å˜ã§ã™ã€‚

ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èª­ã¿è¾¼ã‚€æ–¹æ³•ã¯ã€ä»¥ä¸‹ã®ãƒªãƒ³ã‚¯ã®ã‚ˆã†ã«ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»¥ä¸‹ã«`train, validation`ãªã©ã®splitãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¦ã€ãã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’`load_dataset`é–¢æ•°ã§èª­ã¿è¾¼ã¿ã¾ã™ã€‚

<https://huggingface.co/docs/datasets/create_dataset#folder-based-builders>

<https://huggingface.co/docs/datasets/image_dataset#imagefolder>

<https://huggingface.co/docs/datasets/audio_dataset#audiofolder>

```bash
hoge/dataset_root/train/label/1.png
hoge/dataset_root/train/label/2.png
...
hoge/dataset_root/test/label/1.png
hoge/dataset_root/test/label/2.png
...
```

```python
import datasets as ds

dataset = ds.load_dataset("imagefolder", data_dir="hoge/dataset_root")
```

ã“ã®æ–¹æ³•ã¯ç”»åƒä»¥å¤–ã«ã‚‚`audiofolder`ã¨ã™ã‚‹ã“ã¨ã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

### 2. datasets.Dataset.from_*é–¢æ•°ã‚’åˆ©ç”¨ã™ã‚‹æ–¹æ³•

ã“ã®æ–¹æ³•ã§ã¯ã€ä»¥ä¸‹ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã«ã‚ã‚‹ã‚ˆã†ã«ã€Pythonã®é–¢æ•°ã‚„dictã‚’æ¸¡ã™ã“ã¨ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

<https://huggingface.co/docs/datasets/create_dataset#from-python-dictionaries>

ä¾‹ãˆã°ã€dictã‚’æ¸¡ã™å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ä½œæˆã—ã¾ã™ã€‚

```python
import datasets as ds

dataset_dict = {
    "text": ["hoge", "fuga"]
    "label": [1, 3]
}

dataset = ds.Dataset.from_dict(dataset_dict)
```

generatorã‚’æ¸¡ã™å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ä½œæˆã—ã¾ã™ã€‚å¼•æ•°ã®`gen_kwargs`ã«generatoré–¢æ•°ã«æ¸¡ã™å¼•æ•°ã‚’ã€`num_proc`ã«ä¸¦åˆ—æ•°ã‚’æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
import os
import datasets as ds

dataset_data = [
    {"summary": "hoge", "class": 1},
    {"summary": "fuga", "class": 2},
]

def generator(data):
    for d in data:
        yield {
            "text": d["summary"],
            "label": d["class"]
        }

dataset = ds.Dataset.from_generator(generator,
                                    gen_kwargs={"data": dataset_data},
                                    num_proc=os.cpu_count() // 2)
```

å€‹äººçš„ã«æœ€ã‚‚æ‰‹è»½ãªã®ã¯ã€generatorã¨`datasets.Dataset.from_generator`ã‚’ä½¿ã†æ–¹æ³•ã‹ã¨æ€ã„ã¾ã™ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¸Šã§å…ƒãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢ã‚’ã—ãªãŒã‚‰ä¸¦åˆ—ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆå¯èƒ½ãªã®ã§æ‰‹è»½ã§ãŠã™ã™ã‚ã§ã™ã€‚

### 3. datasets.DatasetBuilderã‚¯ãƒ©ã‚¹ã‚’åˆ©ç”¨ã™ã‚‹æ–¹æ³•

ä»Šå›ç´¹ä»‹ã™ã‚‹æ–¹æ³•ã§ã™ã€‚å¾Œã«è©³ã—ãæ›¸ãã¾ã™ã€‚

## æœ¬é¡Œ: datasets.DatasetBuilderã‚¯ãƒ©ã‚¹ã‚’åˆ©ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆã®æ¦‚è¦

æœ¬è¨˜äº‹ã®æœ¬é¡Œã§ã‚ã‚‹ã€`datasets.DatasetBuilder`ãªã©ã®Builder classã‚’åˆ©ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆæ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ä»Šå›ã¯Builder classã®ä¸­ã§ã‚‚ã€`datasets.GeneratorBasedBuilder`ã‚’ä½¿ã„ã¾ã™ã€‚ã¾ãšã¯Builder classã‚’åˆ©ç”¨ã™ã‚‹ã¨ã©ã‚“ãªåˆ©ç‚¹ãŒã‚ã‚‹ã®ã‹ã‚’ã¾ã¨ã‚ã¦ã‹ã‚‰ã€å®Ÿéš›ã«ä½œæˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

### ãƒ¡ãƒªãƒƒãƒˆ

Builder classã‚’ä½¿ã†ã¨ä»¥ä¸‹ã®ã‚ˆã†ãªåˆ©ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚

- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«è‡ªä½“ã‚’å…¥æ‰‹ã—ã€å‰æº–å‚™ã‚’ã™ã‚‹æ–¹æ³•ã‚’å®šç¾©ã§ãã‚‹
- HuggingFace datasetsã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å½¢å¼ã‚„è¤‡é›‘ãªæº–å‚™ãŒå¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚‚å¯¾å¿œã§ãã‚‹
- è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æŸã­ã‚‹ã“ã¨ãŒã§ãã‚‹

å‰è¿°ã—ãŸ1,2ã®load_datasetã‚„from_generatoré–¢æ•°ã‚’åˆ©ç”¨ã—ãŸæ–¹æ³•ã§ã¯ã€ã‚ã‚‰ã‹ã˜ã‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™ãŒå¿…è¦ã§ã—ãŸãŒã€Builder classã‚’ä½¿ã†ã¨æº–å‚™ã®æ“ä½œã‚‚ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦å®šç¾©ã™ã‚‹ã“ã¨ãŒå¯èƒ½ãªãŸã‚ã€Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ã¿ã‚’å…±æœ‰ã™ã‚‹ã ã‘ã§HuggingFace datasetsã‚’å…±æœ‰ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚

HuggingFaceã§æ—¢ã«å…¬é–‹ã•ã‚Œã¦ã„ã‚‹[JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)ã‚„[JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)ã¨ã„ã£ãŸè¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™ãƒªãƒã‚¸ãƒˆãƒªã¯Builder classã§å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã«ç”¨ã„ã‚‹å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®[wikimedia/wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia)ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å‘¼ã³å‡ºã™ã“ã¨ã§ã€è¨€èªã®åˆ‡ã‚Šæ›¿ãˆã‚’è¡Œã†ã“ã¨ãŒå¯èƒ½ã§ã™ãŒã€Builder classã‚’ç”¨ã„ã‚‹ã¨ã€ã“ã®ã‚ˆã†ãªè¤‡é›‘ãªåˆ†å‰²ã«ã¤ã„ã¦ã‚‚æ‰±ã†ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
from datasets import load_dataset

ds = load_dataset("wikimedia/wikipedia", name="20231101.en")
```

### é–¢é€£class, object

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å®šç¾©ã™ã‚‹ã«å½“ãŸã£ã¦ã€ä»¥ä¸‹ã®ã„ãã¤ã‹ã®é–¢é€£ã‚¯ãƒ©ã‚¹ã‚„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆ©ç”¨ã—ã¾ã™ã€‚

- `datasets.BuilderConfig`
  Builder classã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‘¼ã³å‡ºã™ã®ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åå‰ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚„data_dirãªã©ã‚’å®šç¾©ã—ã¾ã™ã€‚è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æŸã­ã‚‹å ´åˆã¯è¤‡æ•°å®šç¾©ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
- `datasets.DatasetInfo`
  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±ã‚’å®šç¾©ã—ã¾ã™ã€‚citationã‚„homepageã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¦ç´ ã®å‹ãªã©ã‚’å®šç¾©ã—ã¾ã™ã€‚
- `datasets.SplitGenerator`
  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®splitã®å®šç¾©ã‚’ã—ã¾ã™ã€‚ä¾‹ãˆã°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«train, val, testã¨ã„ã£ãŸsplitãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ãã®æ•°ã ã‘å®šç¾©ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
- `datasets.GeneratorBasedBuilder`
  ä¸Šè¨˜3ã¤ã®classã‚’å…ƒã«ã€Datasetã‚’ä½œæˆã™ã‚‹Classã‚’å®šç¾©ã—ã¾ã™ã€‚

## å®Ÿè£…

ãã‚Œã§ã¯å®Ÿéš›ã«`datasets.GeneratorBasedBuilder`ã‚’ä½¿ã£ã¦ã€æ—¥æœ¬èªã‚³ãƒ¼ãƒ‘ã‚¹ã®[livedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹](https://www.rondhuit.com/download.html)ã¨[llm-book/japanese-wikipedia](https://huggingface.co/datasets/llm-book/japanese-wikipedia)ã‚’æŸã­ãŸ`JapaneseTextDataset`ã‚’ä½œæˆã—ã¦ã¿ã¾ã™ã€‚
livedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã¯Webã‹ã‚‰å…¥æ‰‹ã—ã€llm-book/japanese-wikipediaã¯HuggingFaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã‚’ãã®ã¾ã¾åˆ©ç”¨ã—ã¾ã™ã€‚

ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆã¯ä»¥ä¸‹ã¨ã—ã¾ã™ã€‚

```bash
JapaneseTextDataset/
â””â”€â”€ JapaneseTextDataset.py
```

ä»¥ä¸‹ãŒ`JapaneseTextDataset.py`ã®ä¸­èº«ã§ã™ã€‚é †ã«èª¬æ˜ã—ã¦ã„ãã¾ã™ã€‚

```python
from dataclasses import dataclass
from pathlib import Path
from typing import Literal, Optional

import datasets as ds


@dataclass
class LivedoorCorpusConfig(ds.BuilderConfig):
    """BuilderConfig for LivedoorCorpus"""

    def __init__(self, name: str = "livedoor", **kwargs):
        super().__init__(name, **kwargs)


@dataclass
class WikiJPConfig(ds.BuilderConfig):
    """BuilderConfig for WikiJP"""

    def __init__(self, name: str = "wiki", **kwargs):
        super().__init__(name, **kwargs)


class JapaneseTextDataset(ds.GeneratorBasedBuilder):
    """Japanese Text Dataset"""

    VERSION = "1.0.0"
    DEFAULT_CONFIG_NAME = "wiki"
    BUILDER_CONFIGS = [
        WikiJPConfig(
            version=ds.Version(version_str=VERSION), description="Japanese Wikipedia"
        ),
        LivedoorCorpusConfig(
            version=ds.Version(version_str=VERSION), description="Livedoor News Corpus"
        ),
    ]

    def _info(self) -> ds.DatasetInfo:
        if self.config.name == "wiki":
            return ds.DatasetInfo(
                homepage="https://huggingface.co/datasets/llm-book/japanese-wikipedia",
                features=ds.Features(
                    {
                        "text": ds.Value("string"),
                    }
                ),
            )
        elif self.config.name == "livedoor":
            return ds.DatasetInfo(
                homepage="http://www.rondhuit.com/download.html#ldcc",
                features=ds.Features(
                    {
                        "url": ds.Value("string"),
                        "title": ds.Value("string"),
                        "text": ds.Value("string"),
                    }
                ),
            )

    def _split_generators(self, dl_manager: ds.DownloadManager):
        if self.config.name == "wiki":
            dataset = ds.load_dataset(
                "llm-book/japanese-wikipedia",
                trust_remote_code=True,
            )
            return [
                ds.SplitGenerator(
                    name=ds.Split.TRAIN,
                    gen_kwargs={"data": dataset["train"]},
                )
            ]
        elif self.config.name == "livedoor":
            file_path = dl_manager.download_and_extract(
                "http://www.rondhuit.com/download/ldcc-20140209.tar.gz"
            )
            return [
                ds.SplitGenerator(
                    name=ds.Split.TRAIN,
                    gen_kwargs={"file_path": file_path, "split": "train"},
                ),
                ds.SplitGenerator(
                    name=ds.Split.TEST,
                    gen_kwargs={"file_path": file_path, "split": "test"},
                ),
            ]

    def _generate_examples(
        self,
        data: Optional[ds.Dataset] = None,
        file_path: Optional[str] = None,
        split: Literal["train", "test"] | None = None,
    ):
        if self.config.name == "wiki":
            if data is None:
                raise ValueError("data must be specified")
            for i, example in enumerate(data):
                yield i, {"text": example["text"]}
        elif self.config.name == "livedoor":
            if file_path is None:
                raise ValueError("file_path must be specified")
            if split is None:
                raise ValueError("split must be specified")

            paths = [
                p
                for p in (Path(file_path) / "text").glob("*/*.txt")
                if p.name != "LICENSE.txt"
            ]
            if split == "train":
                paths = paths[: int(len(paths) * 0.8)]
            else:
                paths = paths[int(len(paths) * 0.8) :]
            for i, path in enumerate(paths):
                with open(path, "r", encoding="utf-8") as f:
                    url = f.readline().strip()
                    title = f.readline().strip()
                    text = f.read()
                yield i, {"url": url, "title": title, "text": text}
```

### datasets.BuilderConfig

ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ã®ä»¥ä¸‹ã®éƒ¨åˆ†ã§ã€Livedoorã¨Wikipediaã®BuilderConfigã‚’å®šç¾©ã—ã¦ã„ã¾ã™ã€‚

```python
@dataclass
class LivedoorCorpusConfig(ds.BuilderConfig):
    """BuilderConfig for LivedoorCorpus"""

    def __init__(self, name: str = "livedoor", **kwargs):
        super().__init__(name, **kwargs)


@dataclass
class WikiJPConfig(ds.BuilderConfig):
    """BuilderConfig for WikiJP"""

    def __init__(self, name: str = "wiki", **kwargs):
        super().__init__(name, **kwargs)
```

ä»Šå›ã¯ç‰¹åˆ¥ãªã“ã¨ã¯ã—ã¦ã„ã¾ã›ã‚“ãŒã€`load_dataset`ã‚’å‘¼ã¶ã¨ãã«å¼•æ•°ã‚’è¿½åŠ ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã—ã¾ã™ã€‚

```python
@dataclass
class WikiJPConfig(ds.BuilderConfig):
    """BuilderConfig for WikiJP"""

    def __init__(self, name: str = "wiki", language: str = "ja", **kwargs):
        super().__init__(name, **kwargs)
        self.language = "ja"
```

ä¸Šè¨˜ã®ä¾‹ã§ã¯`language`ã¨ã„ã†å¼•æ•°ã‚’è¶³ã—ã¦ã„ã¾ã™ã€‚ã“ã†ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€æ™‚ã«ä»¥ä¸‹ã®ã‚ˆã†ãªæ–¹æ³•ã§`language`å¼•æ•°ã‚’æ¸¡ã™ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```python
dataset = ds.load_dataset("JapaneseTextDataset", name="wiki", language="ja")
```

### datasets.GeneratorBasedBuilder

`datasets.GeneratorBasedBuilder`ã‚’ç¶™æ‰¿ã—ãŸ`JapaneseTextDataset`ã‚¯ãƒ©ã‚¹ã«ã¯`VERSION`, `DEFAULT_CONFIG_NAME`,  `BUILDER_CONFIGS`ã‚’è¨­å®šã—ã¾ã™ã€‚
è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‘¼ã¹ã‚‹ã‚ˆã†ã«ã™ã‚‹å ´åˆã¯`BUILDER_CONFIGS`ã«è¤‡æ•°ã®`datasets.BuilderConfig`ã‚’ãƒªã‚¹ãƒˆã§åˆ—æŒ™ã—ã¦ãŠãã¾ã™ã€‚
`DEFAULT_CONFIG_NAME`ã§ã¯load_datasetã§ä½•ã‚‚æŒ‡å®šã—ãªã‹ã£ãŸå ´åˆã«ã€ã©ã®config_nameãŒå‘¼ã°ã‚Œã‚‹ã‹ã‚’è¨­å®šã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯`"default"`ã§ã™ã€‚

```python
class JapaneseTextDataset(ds.GeneratorBasedBuilder):
    """Japanese Text Dataset"""

    VERSION = "1.0.0"
    DEFAULT_CONFIG_NAME = "wiki"
    BUILDER_CONFIGS = [
        WikiJPConfig(
            version=ds.Version(version_str=VERSION), description="Japanese Wikipedia"
        ),
        LivedoorCorpusConfig(
            version=ds.Version(version_str=VERSION), description="Livedoor News Corpus"
        ),
    ]
```

### datasets.GeneratorBasedBuilder::_info

`datasets.GeneratorBasedBuilder`ã®`_info`é–¢æ•°ã§ã¯ã€`datasets.DatasetInfo`å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±ã‚’è¿”ã—ã¦ã‚ã’ã¾ã™ã€‚ã“ã®ã¨ãã€`self.config`ã§ã€`BUILDER_CONFIGS`ã®å†…å®¹ã‚’å‚ç…§ã§ãã‚‹ã®ã§ã€å‘¼ã³å‡ºã™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±ã‚’å‹•çš„ã«å¤‰æ›´ã§ãã¾ã™ã€‚

```python
def _info(self) -> ds.DatasetInfo:
    if self.config.name == "wiki":
        return ds.DatasetInfo(
            homepage="https://huggingface.co/datasets/llm-book/japanese-wikipedia",
            features=ds.Features(
                {
                    "text": ds.Value("string"),
                }
            ),
        )
    elif self.config.name == "livedoor":
        return ds.DatasetInfo(
            homepage="http://www.rondhuit.com/download.html#ldcc",
            features=ds.Features(
                {
                    "url": ds.Value("string"),
                    "title": ds.Value("string"),
                    "text": ds.Value("string"),
                }
            ),
        )
```

### datasets.GeneratorBasedBuilder::_split_generators

`datasets.GeneratorBasedBuilder`ã®`_split_generators(self, dl_manager: ds.DownloadManager)`é–¢æ•°ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã©ã®ã‚ˆã†ãªsplitãŒã‚ã‚‹ã®ã‹ã‚’`ds.SplitGenerator`ã§å®šç¾©ã—ã¦ãƒªã‚¹ãƒˆã§è¿”ã—ã¾ã™ã€‚`SplitGenerator`ã®`gen_kwargs`ã¯å¾Œã»ã©å‡ºã¦ãã‚‹ã€ãƒ‡ãƒ¼ã‚¿1è¡Œã‚’è¿”ã™`_generate_examples`ã«æ¸¡ã™å¼•æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚

ä»Šå›ã¯ã€`"llm-book/japanese-wikipedia"`ã¯HuggingFaceã‹ã‚‰å–å¾—ã™ã‚‹ã®ã§å†…éƒ¨ã§load_datasetã‚’å‘¼ã‚“ã§ã„ã¾ã™ã€‚
Livedoor News Corpusã¯ã‚¦ã‚§ãƒ–ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ©ç”¨ã™ã‚‹ã®ã§ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§æ›¸ãã€ä¿å­˜å…ˆã‚’`gen_kwargs`ã«æ¸¡ã—ã¦ã‚ã‚Šã¾ã™ã€‚
`dl_manager.download_and_extract`ã‚’ä½¿ã†ã¨ã€URLã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨å›ç­”ã‚’ä¸€æ°—ã«è¡Œã£ã¦ãã‚Œã‚‹ã®ã§ã“ã‚Œã‚’åˆ©ç”¨ã—ã¾ã™ã€‚

```python
def _split_generators(self, dl_manager: ds.DownloadManager):
    if self.config.name == "wiki":
        dataset = ds.load_dataset(
            "llm-book/japanese-wikipedia",
            trust_remote_code=True,
        )
        return [
            ds.SplitGenerator(
                name=ds.Split.TRAIN,
                gen_kwargs={"data": dataset["train"]},
            )
        ]
    elif self.config.name == "livedoor":
        file_path = dl_manager.download_and_extract(
            "http://www.rondhuit.com/download/ldcc-20140209.tar.gz"
        )
        return [
            ds.SplitGenerator(
                name=ds.Split.TRAIN,
                gen_kwargs={"file_path": file_path, "split": "train"},
            ),
            ds.SplitGenerator(
                name=ds.Split.TEST,
                gen_kwargs={"file_path": file_path, "split": "test"},
            ),
        ]
```

### datasets.GeneratorBasedBuilder::_generate_examples

`datasets.GeneratorBasedBuilder`ã®`_generate_examples`é–¢æ•°ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®1ãƒ‡ãƒ¼ã‚¿ã‚’ã©ã®ã‚ˆã†ã«è¿”ã™ã‹ã‚’å®šç¾©ã—ã¾ã™ã€‚å¼•æ•°ã«ã¯å…ˆã»ã©ã®`_split_generators`ã§å®šç¾©ã—ãŸ`gen_kwargs`ãŒæ¸¡ã•ã‚Œã¾ã™ã€‚generatorã§å®šç¾©ã™ã‚‹ã®ã§ã€`yield`æ§‹æ–‡ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚

```python
def _generate_examples(
        self,
        data: Optional[ds.Dataset] = None,
        file_path: Optional[str] = None,
        split: Literal["train", "test"] | None = None,
    ):
        if self.config.name == "wiki":
            if data is None:
                raise ValueError("data must be specified")
            for i, example in enumerate(data):
                yield i, {"text": example["text"]}
        elif self.config.name == "livedoor":
            if file_path is None:
                raise ValueError("file_path must be specified")
            if split is None:
                raise ValueError("split must be specified")

            paths = [
                p
                for p in (Path(file_path) / "text").glob("*/*.txt")
                if p.name != "LICENSE.txt"
            ]
            if split == "train":
                paths = paths[: int(len(paths) * 0.8)]
            else:
                paths = paths[int(len(paths) * 0.8) :]
            for i, path in enumerate(paths):
                with open(path, "r", encoding="utf-8") as f:
                    url = f.readline().strip()
                    title = f.readline().strip()
                    text = f.read()
                yield i, {"url": url, "title": title, "text": text}
```

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‘¼ã³å‡ºã—

ä»¥ä¸Šã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰ã¯çµ‚äº†ã§ã™ã€‚å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‘¼ã³å‡ºã—ã¦ã¿ã¾ã™ã€‚
ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å‘¼ã³å‡ºã›ã¾ã™ã€‚

```python
import datasets as ds

# Load the dataset
dataset = ds.load_dataset("./JapaneseTextDataset", "wiki", trust_remote_code=True)
print(dataset)

# Load the dataset
dataset = ds.load_dataset(
    "./JapaneseTextDataset", "livedoor", trust_remote_code=True
)
print(dataset)
```

å®Ÿè¡Œçµæœã¯ä»¥ä¸‹ã§ã™ã€‚

```bash
ja_wiki.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.37G/6.37G [00:53<00:00, 38.7MB/s]
Generating train split: 1363395 examples [00:38, 35532.80 examples/s]
Generating train split: 1363395 examples [03:22, 6731.14 examples/s]
DatasetDict({
    train: Dataset({
        features: ['text'],
        num_rows: 1363395
    })
})
Downloading data: 31.6MB [00:00, 50.9MB/s]
Generating train split: 5893 examples [00:01, 3227.36 examples/s]
Generating test split: 1474 examples [00:00, 2580.49 examples/s]
DatasetDict({
    train: Dataset({
        features: ['url', 'title', 'text'],
        num_rows: 5893
    })
    test: Dataset({
        features: ['url', 'title', 'text'],
        num_rows: 1474
    })
})
```

ç„¡äº‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå‘¼ã³å‡ºã›ã¾ã—ãŸã€‚

## HuggingFace Hubã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’HuggingFaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€å…ˆã»ã©ä½œæˆã—ãŸ`JapaneseTextDataset`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãã®ã¾ã¾pushã™ã‚Œã°è‰¯ã„ã§ã™ã€‚

ä»¥ä¸‹ã®Templateã«ã‚ã‚‹ã‚ˆã†ã«ã€`README.md`ã«Datasetã®æƒ…å ±ã‚’æ›¸ã„ã¦ãŠãã¨ãã®ã¾ã¾ä½¿ã‚ã‚Œã¾ã™ã€‚

<https://github.com/huggingface/datasets/tree/main/templates>

ã¾ãŸã€ä»¥ä¸‹ã®ãƒªãƒã‚¸ãƒˆãƒªã¯JGLUEã‚’HuggingFaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹GitHubã®ãƒªãƒã‚¸ãƒˆãƒªãªã®ã§ã™ãŒã€CI/CDã¾ã§çµ„ã‚“ã§ã‚ã‚Šã€å¤§å¤‰å‚è€ƒã«ãªã‚Šã¾ã™ã€‚

<https://github.com/shunk031/huggingface-datasets_JGLUE/tree/main>

ä»¥ä¸Šã€ç°¡å˜ã«ã§ã™ãŒBuilder classã‚’ä½¿ã£ãŸHuggingFace datasetsã®ä½œæˆæ–¹æ³•ã§ã—ãŸã€‚

## å‚è€ƒ

[https://github.com/shunk031/huggingface-datasets_JGLUE/tree/main](https://github.com/shunk031/huggingface-datasets_JGLUE/tree/main)

[https://qiita.com/oyahiroki/items/662ed10ab3f8d4bf8f61](https://qiita.com/oyahiroki/items/662ed10ab3f8d4bf8f61)

[https://github.com/huggingface/datasets/tree/main/templates](https://github.com/huggingface/datasets/tree/main/templates)

[https://huggingface.co/docs/datasets/package_reference/builder_classes](https://huggingface.co/docs/datasets/package_reference/builder_classes)

[https://huggingface.co/datasets/sbintuitions/JMTEB/blob/main/JMTEB.py](https://huggingface.co/datasets/sbintuitions/JMTEB/blob/main/JMTEB.py)
